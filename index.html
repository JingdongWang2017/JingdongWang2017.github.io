
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=GB2312">
<meta name="keywords" content="Jingdong Wang">
<meta name="description" content="Academic homepage of Jingdong Wang">
<!--<link href="CSS/bootstrap.min.css" media="all" rel="stylesheet">-->
<link href="CSS/main.css" media="all" rel="stylesheet">
<title>Jingdong Wang</title>
</head>

<body>

<!--<NAV class="navbar navbar-default navbar-fixed-top">
    <tr> <td></td><td align="right"><A  href="#publications">Publications</A> </td></tr>
</NAV>-->
<!--<NAV class="navbar navbar-default navbar-fixed-top" id="navbar-main">
<DIV class="container">
<DIV class="navbar-header">
<BUTTON class="navbar-toggle collapsed" aria-expanded="false" type="button"
data-toggle="collapse" data-target=".navbar-collapse"><SPAN class="sr-only">Toggle navigation</SPAN>
<SPAN class="icon-bar"></SPAN><SPAN class="icon-bar"></SPAN><SPAN
class="icon-bar"></SPAN></BUTTON>
 </DIV>
<DIV class="collapse navbar-collapse" id="#navbar-collapse-1"> <UL
class="nav navbar-nav navbar-right">
  <LI class="nav-item"><A
  href="">Home</A></LI>
  <LI class="nav-item"><A
  href="">Publications</A></LI>
  <LI class="nav-item"><A
  href="">Teaching</A></LI>
  </UL></DIV></DIV></NAV>-->


<section id ="headinformation" class="sectionwhite">
<table id="tbInformation" width="100%">
    <tr>
         <td width="600">
         <h1>Jingdong Wang  <font style ="font-family:Microsoft JhengHei ">(王井东)</font>, Fellow of IEEE and IAPR</h1>
         </td>
         <td rowspan="2" align="right">
	      <img src="images/WangJingdong.jpg" border="0" height="200">
         </td>
    </tr>
    <tr>

	    <td>
	           <p><font size="4"> Chief Architect for Computer Vision <font size = "2" color = "gray"><del>Senior Principal Research Manager</del></font><br>
	           AI Group, Baidu.  <font size = "2" color = "gray"><del>Microsoft Research</del></font> <br>            
	           Email_1: wangjingdong at baidu dot com <font size = "2" color = "gray"><del>jingdw at microsoft dot com</del></font> <br>
	           Email_2: welleast at outlook dot com </font></p>
	           </td>
	    
	</tr>
	<tr><td>
	<A href="CV_Jingdong_Wang.pdf" target="_blank">CV</A>&nbsp&nbsp&nbsp
	<A href="http://scholar.google.com/citations?user=z5SPCmgAAAAJ&hl=en" target="_blank">Google Scholar</A>&nbsp&nbsp&nbsp
	<A href="https://dblp.uni-trier.de/pid/49/3441.html" target="_blank">DBLP</A>&nbsp&nbsp&nbsp
	<A href="https://orcid.org/0000-0002-4888-4445" target="_blank">ORCID</A></td></tr>
</table>
</SECTION>




<!--<SECTION id ="Biography" class="sectiongray"  > -->
<SECTION id ="Biography" class="sectionwhite">
<h3> Biography </h3>
<font size ="normal">
Jingdong Wang is Chief Architect for computer vision with
	the Artificial Intelligence Group at Baidu. Before joining Baidu, he was a Senior Principal Researcher at Microsoft Research Asia from September 2007 to August 2021.
<!--I received the B.Eng. and M.Eng. degrees in Automation
	from the <a href="http://www.au.tsinghua.edu.cn/" target="_blank">
	Department of Automation</a>,
	<a href="http://www.tsinghua.edu.cn/qhdwzy/index.jsp" target="_blank" >
	Tsinghua University</a>, Beijing, China, in 2001 and 2004, respectively,
	and the PhD degree in Computer Science from the
	<a href="http://www.cse.ust.hk/" target="_blank">Department of Computer Science and Engineering</a>, the
	<a href="http://www.ust.hk/" target="_blank">Hong Kong University of
	Science and Technology</a>, Hong Kong, in 2007.-->
His areas of interest include <i>neural architecture design,
human pose estimation,
semantic segmentation,
image classification,
object detection,
large-scale indexing,
and salient object detection</i>.
He has been serving/served as an Associate Editor of IEEE TPAMI, IJCV, IEEE TMM, and IEEE TCSVT,
and an area chair of leading conferences in vision, multimedia, and AI, such as CVPR, ICCV, ECCV, ACM MM, IJCAI, and AAAI.
He was elected as an ACM Distinguished Member, a Fellow of IAPR, and a Fellow of IEEE,
for his contributions to visual content understanding and retrieval.</font>
<A  href="CV_Jingdong_Wang.pdf" target="_blank">CV</A>
<br><br>
</SECTION>
<!--<div><hr></div>-->
<!--style="background-color:rgb(247,247,247)"-->

<SECTION id ="whatisnew"  class="sectionwhite">
<h3> Recent news</h3>
<font size ="normal">

<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >

<TR> <td>
28. <font color="blue"> ICLR 2022 spotlight. 
Transformer does not outperform CNN: On the Connection between Local Attention and Dynamic Depth-wise Convolution. </font>
<a href="https://arxiv.org/abs/2106.04263" target="_blank">[pdf]</a>
<a href="https://github.com/Atten4Vis/DemystifyLocalViT" target="_blank">[code].</a>
3/2022
</td> </TR>
	
<TR> <td>
27.  <font color="blue"> People of ACM interview: <a href="https://www.acm.org/articles/people-of-acm/2021/jingdong-wang" target="_blank">URL</a> 12/2021. </font>
</td> </TR>
	
<TR> <td>
26.  <font color="blue"> Elected as Fellow of IEEE, for his contributions to visual content understanding and retrieval, 11/2021. </font>
</td> </TR>

<TR> <td>
25. <font> Code released for our NeurIPS 2021 paper,
HRFormer: High-Resolution Transformer for Dense Prediction. </font>
<a href="https://arxiv.org/pdf/2110.09408.pdf" target="_blank">[pdf]</a>
<a href="https://github.com/HRNet/HRFormer" target="_blank">code</a>. 09/2021
</td> </TR>
	
	
<TR> <td>
24. <font> Code released for our NeurIPS 2021 paper,
SPANN: Highly-efficient Billion-scale ApproximateNearest Neighbor Search. </font>
<a href="http://arxiv.org/pdf/2111.08566.pdf" target="_blank">[pdf]</a>
<a href="https://github.com/microsoft/SPTAG" target="_blank">code</a>. 09/2021
</td> </TR>
	
	
<TR> <td>
23. <font> Code released for our ICCV 2021 paper,
Conditional DETR for Fast Training Convergence. </font>
<a href="https://arxiv.org/abs/2108.06152" target="_blank">[pdf]</a>
<a href="https://github.com/Atten4Vis/ConditionalDETR" target="_blank">code</a>. 8/16/2021
</td> </TR>


<TR> <td>
22. <font> Local Transformer attention is equivalent to inhomogeneous
dynamic depth-wise convolution:</font>
<a href="Pubs/Demystify_Local_Attention.pdf" target="_blank">Demystifying local attention.</a> 7/2021
</td> </TR>


<TR> <td>
21. <font> Welcome to the large scale approximate nearest search challenge at NeurIPS 2021:</font>
<a href="http://big-ann-benchmarks.com/index.html" target="_blank">Big ANN Benchmark.</a> 5/2021
</td> </TR>

<TR> <td>
20. <font> HRNet is shipped to <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/form-recognizer/whats-new?tabs=csharp#may-2021" target="_blank">Form Recognizer</a>
for Table Recognition. </font> 5/2021
</td> </TR>

<TR> <td>
19. <font> Update object-contextual representation for semantic segmentation (ECCV 2020). 
We rephrase it as Segmentation Transformer.</font>
<a href="https://arxiv.org/pdf/1909.11065.pdf" target="_blank">[pdf]</a>
<a href="https://github.com/HRNet/HRNet-Semantic-Segmentation" target="_blank">code</a>. 5/4/2021
</td> </TR>

<TR> <td>
18. <font> Code released for our CVPR 2021 paper,
Lite-HRNet: A Lightweight High-Resolution Network.</font>
<a href="https://arxiv.org/pdf/2104.06403" target="_blank">[pdf]</a>
<a href="https://github.com/HRNet/Lite-HRNet" target="_blank">code</a>. 4/12/2021
</td> </TR>

<TR> <td>
17. <font> Code released for our CVPR 2021 paper,
Bottom-Up Human Pose Estimation via Disentangled Keypoint Regression.</font>
<a href="https://arxiv.org/pdf/2104.02300.pdf" target="_blank">[pdf]</a>
<a href="https://github.com/HRNet/DEKR" target="_blank">code</a>. 4/7/2021
</td> </TR>


<TR> <td>
16. HRNet: Deep High-Resolution Representation Learning for Visual Recognition. Accepted by TPAMI. 
[<a href="Pubs/TPAMI-HRNet.pdf" target="_blank">pdf</a>] or [<a href="https://arxiv.org/pdf/1908.07919.pdf" target="_blank">pdf at arXiv</a>].
This is a longer version of <a href="https://arxiv.org/abs/1902.09212" target="_blank">the HRNet paper</a> published in CVPR 2019.
HRNet is a stronger backbone, and acheives superior performance
on human pose estimation, semantic segmentation, object detection, face alignment, and so on. 
Codes are available.
Human pose estimation: 
<iframe src="https://ghbtns.com/github-btn.html?user=leoxiaobin&repo=deep-high-resolution-net.pytorch&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>;
Semantic segmentation <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Semantic-Segmentation&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>;
Object detection <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Object-Detection&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>;
Facial landmark detection <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Facial-Landmark-Detection&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>;
ImageNet classification:
<iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Image-Classification&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>. 3/13/2020
</td> </TR>



<TR> <td>
15. HRNet + OCR + SegFix is ranked 1 on cityscapes segmentation. <a href="https://www.cityscapes-dataset.com/benchmarks/" target="_blank">Cityscapes segmentation leaderboard</a> (January2020).
The implementation of HRNet + OCR is available: <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR" target="_blank">code</a>
</td> </TR>


<TR> <td>
14. Invited as an area chair of CVPR 2020, ECCV 2020, and IJCAI 2020.
</td> </TR>


<TR> <td>
13. HRNet + OCR is ranked 1 on cityscapes segmentation. <a href="https://www.cityscapes-dataset.com/benchmarks/" target="_blank">Cityscapes segmentation leaderboard</a> (July 2019).
</td> </TR>

<!--<TR> <td>
13. <font color="red"> HRNet talk video</font> available:
<a href="https://1drv.ms/v/s!AjTILtw5fdWhg8RDz-S2oXcIsHHQ6g?e=asEQrp" target="_blank">OneDrive</a>.
</td> </TR>-->

<!--<a href="https://www.youtube.com/watch?v=sIP3MrFWCpg" target="_blank">at YouTube</a> -->



<TR> <td>
12. <font>High-Resolution Network (HRNet). A replacement of classification networks for visual recognition.&nbsp<a href="./Projects/HRNet/index.html" target="_blank">projects page.</a>
</font>
</td> </TR>

<TR> <td>
11. Fast neighborhood graph-based approximate nearest neighbor search: <a href="https://github.com/Microsoft/SPTAG" target="_blank">code</a>
<iframe src="https://ghbtns.com/github-btn.html?user=Microsoft&repo=SPTAG&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>.
<a href="https://blogs.microsoft.com/ai/bing-vector-search/?utm_source=pre-amp&utm_campaign=81516/" target="_blank">Bing vector search</a>.
<a href="https://techcrunch.com/2019/05/15/microsoft-open-sources-a-crucial-algorithm-behind-its-bing-search-services/" target="_blank">TechCrunch</a>.
</td> </TR>


<TR> <td>
10. Invited as an area chair of ICCV 2019, and IJCAI 2019.
</td> </TR>

<TR> <td>
9. Elected as an ACM Distinguished Member, 11/2018.
</td> </TR>

<TR> <td>
8. Gave a keynote talk about approximate nearest neighbor search on 9/29/2018 at JD.com.
&nbsp;<a href="./Projects/ANN/ANN.pdf" target="_blank">slides</a>
</td> </TR>


<TR> <td>
7. Second place entry, COCO keypoints detection challenge ECCV 2018.
</td> </TR>

<TR> <td>
6. Appointed as AE of TPAMI, 09/2018.
</td> </TR>

<TR> <td>
5. Elected as Fellow of IAPR 2018.
</td> </TR>


<TR> <td>
4. One paper is accepted by ECCV 2018.
</td> </TR>

<TR> <td>
3. Two papers are accepted by ACM MM 2018.
</td> </TR>


<TR> <td>
2. Three papers are accepted by CVPR 2018.
</td> </TR>

<TR> <td>
1. Appointed as AE of TCSVT, 01/2018.
</td> </TR>

</table>
<br>
</font>
</SECTION>


<SECTION id ="codesandatasets"  class="sectionwhite">
<h3> Codes and datasets</h3>
<font size ="normal">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >

<TR> <td>
1. <b>High-resolution networks (HRNet)</b>.
A replacement of classification networks for computer vision problems&nbsp<a href="./Projects/HRNet/index.html" target="_blank">projects</a>.
</font>
 <a href="https://arxiv.org/abs/1902.09212" target="_blank">Human pose estimation (CVPR 2019)</a>: <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank">code</a>
<iframe src="https://ghbtns.com/github-btn.html?user=leoxiaobin&repo=deep-high-resolution-net.pytorch&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>.
Other applications <a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank">pdf (short)</a>
<a href="https://arxiv.org/pdf/1908.07919.pdf" target="_blank">pdf (long)</a> <a href="https://github.com/HRNet" target="_blank">code</a>:
semantic segmentation <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Semantic-Segmentation&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>,
object detection <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Object-Detection&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>,
facial landmark detection <iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Facial-Landmark-Detection&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>,
and ImageNet classification
<iframe src="https://ghbtns.com/github-btn.html?user=HRNet&repo=HRNet-Image-Classification&type=star&count=true" frameborder="0" scrolling="0" width="85px" height="20px"></iframe>.
</td>
</TR>




<TR> <td>
2. <b>Small convolutional neural networks</b>.
Interleaved group convolutions. IGCV1 (ICCV 2017):
&nbsp;<a href="https://arxiv.org/pdf/1707.02725.pdf" target="_blank">pdf</a>
&nbsp;<a href="https://github.com/hellozting/InterleavedGroupConvolutions" target="_blank">code</a> |
IGCV2 (CVPR 2018):
&nbsp;<a href="https://arxiv.org/pdf/1804.06202.pdf" target="_blank">pdf</a> |
IGCV3 (BMVC 2018):
&nbsp;<a href="https://arxiv.org/abs/1806.00178" target="_blank">pdf </a>
&nbsp;<a href="https://github.com/homles11/IGCV3" target="_blank">code</a>
<iframe src="https://ghbtns.com/github-btn.html?user=homles11&repo=IGCV3&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>
<!--&nbsp;<a href="https://arxiv.org/pdf/1804.06202.pdf" target="_blank">Related papers</a>-->
</td>
</TR>

<TR> <td>
3.  <b>Large-scale indexing for similarity search</b>.
Neighborhood graph search (ACM MM 2012):
&nbsp;<a href="Pubs\ACMMM12-GraphSearch.pdf" target="_blank">pdf</a> |
Neighborhood graph construction (CVPR 2012):
&nbsp;<a href="Pubs\CVPR12-GraphConstruction.pdf" target="_blank">pdf</a> |
Trinary-projection trees (TPAMI, CVPR 2010):
&nbsp;<a href="Pubs\TPAMI-TPTree.pdf" target="_blank">pdf</a> |
<a href="https://github.com/Microsoft/SPTAG" target="_blank">code</a>
<iframe src="https://ghbtns.com/github-btn.html?user=Microsoft&repo=SPTAG&type=star&count=true" frameborder="0" scrolling="0" width="94px" height="20px"></iframe>
</td>
</TR>
<!--<br><br>-->


<TR> <td>
4.  <b>Hashing and quantization</b>.
A survey on learning to hash (TPAMI):
&nbsp;<a href="Pubs\LTHSurvey.pdf" target="_blank">pdf v2</a>
&nbsp;<a href="Pubs\LTHSurvey.html" target="_blank">html v2</a>
&nbsp;<a href="https://github.com/welleast/Learning2Hash" target="_blank">tex v2</a>
&nbsp;<a href="Pubs\HashingSurvey-August-13-2014.pdf" target="_blank">pdf v1</a> |
Composite quantization (TPAMI, ICML 2014):
&nbsp;<a href="https://arxiv.org/abs/1712.00955" target="_blank">pdf</a>
&nbsp;<a href="https://github.com/hellozting/CompositeQuantization" target="_blank">code</a>
</td>
</TR>
<!--<br><br>-->



<TR> <td>
5. <b>Salient object detection</b>.
Discriminative Regional Feature Integration (IJCV, CVPR 2013):
&nbsp;<a href="Pubs\CVPR13-Saliency.pdf" target="_blank">pdf (CVPR)</a>
&nbsp;<a href="https://link.springer.com/article/10.1007/s11263-016-0977-3" target="_blank">pdf (IJCV)</a>
&nbsp;<a href="https://github.com/playerkk/drfi_cpp" target="_blank">c++ code</a>
&nbsp;<a href="https://github.com/playerkk/drfi_matlab" target="_blank">matlab code</a>
<!--&nbsp;<a href="http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip" target="_blank">MSRA-B</a>-->
&nbsp;<a href="https://github.com/playerkk/drfi_matlab" target="_blank">project</a> |
Local context (BMVC 2011):
&nbsp;<a href="Pubs\BMVC11-Salient.pdf" target="_blank">pdf</a>
&nbsp;<a href="https://people.cs.umass.edu/~hzjiang/files/CBSaliency-release.zip" target="_blank">code</a> |
Learning to detect a salient object (TPAMI):
&nbsp;<a href="Pubs\TPAMI2011-Saliency" target="_blank">pdf</a>
<!--&nbsp;<a href="http://mftp.mmcheng.net/Data/MSRA10K_Imgs_GT.zip" target="_blank">MSRA10K</a>-->
</td>
</TR>
</table>
</font>

<br>
</SECTION>
<!--<!--<div><hr></div>-->
<!--style="background-color:rgb(247,247,247)"-->


<!--<!--<!----><SECTION id="Publications" class="sectiongray" >
<SECTION id ="Biography" class="sectionwhite">
<tr><h3> Recent publications [<a href="https://dblp.uni-trier.de/pid/49/3441.html" target="_blank">DBLP</a>]</h3>  </tr>
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >

<!--<tr><td>&nbsp</td></tr>-->

<TR> <TD class="pubindex"><FONT>[14]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">HRFormer: High-Resolution Transformer for Dense Prediction. </FONT>
<FONT >Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen,  and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">NeurIPS 2021.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/2110.09408.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/HRNet/HRFormer" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>

<TR> <TD class="pubindex"><FONT>[13]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">SPANN: Highly-efficient Billion-scale ApproximateNearest Neighbor Search. </FONT>
<FONT >Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang,  and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">NeurIPS 2021.</FONT>
<FONT >&nbsp;[<a href="http://arxiv.org/pdf/2111.08566.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/microsoft/SPTAG" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>

<TR> <TD class="pubindex"><FONT>[12]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Conditional DETR for Fast Training Convergence. </FONT>
<FONT >Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">ICCV 2021.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/abs/2108.06152" target="_blank">pdf</a>]
[<a href="https://github.com/Atten4Vis/ConditionalDETR" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>


<TR> <TD class="pubindex"><FONT>[11]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Object-Contextual Representations for Semantic Segmentation. </FONT>
<FONT >Yuhui Yuan, Xilin Chen, and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">ECCV 2020.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/1909.11065.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>



<TR> <TD class="pubindex"><FONT>[10]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Deep high-resolution representation learning for human pose estimation. </FONT>
<FONT >Ke Sun, Bin Xiao, Dong Liu and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">CVPR 2019.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/abs/1902.09212" target="_blank">pdf</a>]
[<a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>



<TR> <TD class="pubindex"><FONT>[9]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Part-Aligned Bilinear Representations for Person Re-identification. </FONT>
<FONT >Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung Mu Lee. </FONT>
<FONT class="pubbooktitle">ECCV 2018.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/1804.07094.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/yuminsuh/part_bilinear_reid" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>

<TR> <TD class="pubindex"><FONT>[8]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural
Networks. </FONT>
<FONT >Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang </FONT>
<FONT class="pubbooktitle">BMVC 2018.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/abs/1806.00178" target="_blank">pdf</a>]
[<a href="https://github.com/homles11/IGCV3" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>


<TR> <TD class="pubindex"><FONT>[7]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Composite Quantization. </FONT>
<FONT >Jingdong Wang, and Ting Zhang. </FONT>
<FONT class="pubbooktitle">TPAMI 2018.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/abs/1712.00955" target="_blank">pdf</a>]
[<a href="https://github.com/hellozting/CompositeQuantization" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>




<TR> <TD class="pubindex"><FONT>[6]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Deep Convolutional Neural Networks with Merge-and-Run Mappings. </FONT>
<FONT >Liming Zhao, Mingjie Li, Depu Meng, Xi Li, Zhaoxiang Zhang, Yueting Zhuang, Zhuowen Tu, and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">IJCAI 2018.</FONT>
<FONT >&nbsp;[<a href="href="" target="_blank">pdf</a>]
[<a href="https://github.com/zlmzju/fusenet" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>



<TR> <TD class="pubindex"><FONT>[5]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks. </FONT>
<FONT >Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, and Guo-Jun Qi. </FONT>
<FONT class="pubbooktitle">CVPR 2018.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/1804.06202.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/homles11/IGCV3" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>


<TR> <TD class="pubindex"><FONT>[4]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">IGCV1: Interleaved Group Convolutions. </FONT>
<FONT >Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">ICCV 2017.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/1707.02725.pdf" target="_blank">pdf</a>]
&nbsp;[<a href="https://github.com/hellozting/InterleavedGroupConvolutions" target="_blank">code</a>]
&nbsp;[<a href="http://www.arxiv-sanity.com/1707.02725v2" target="_blank">related papers</a>]
&nbsp;[<a href="https://zhuanlan.zhihu.com/p/28531708" target="_blank">Zhihu</a>]
&nbsp;[<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650729423&idx=5&sn=c7675bb34df25f0f3ded3efd347190a9&chksm=871b2fb1b06ca6a73bd7fd8c2e83bc100a65dcb56d865f44d858b7fa1d3c07f0115a1c11d112&mpshare=1&scene=1&srcid=0731jxyN8VTce83yHYWIJfbo&pass_ticket=6a4my5wGf1dozgLJbT9QePWBhKxxrg%2FvbLeJzOALNQedJwDcZFuzDi0yt%2B9ogPbF" target="_blank">blog</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>

<TR> <TD class="pubindex"><FONT>[3]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Deeply-Learned Part-Aligned Representations for Person Re-Identification. </FONT>
<FONT >Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. </FONT>
<FONT class="pubbooktitle">ICCV 2017.</FONT>
<FONT >&nbsp;[<a href="https://arxiv.org/pdf/1707.07256.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/zlmzju/part_reid" target="_blank">code</a>]
<font color="#FF0000"> </FONT> </FONT>
</TD></TR>





<TR> <TD class="pubindex"><FONT>[2]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">A Survey on Learning to Hash. </FONT>
<FONT >Jingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, and Heng Tao Shen. </FONT>
<FONT class="pubbooktitle">TPAMI, Accepted 2017.</FONT>
<FONT >&nbsp;[<a href="Pubs\LTHSurvey.pdf" target="_blank">pdf v2</a>]
&nbsp;[<a class="style6" href="Pubs\HashingSurvey-August-13-2014.pdf" target="_blank">[pdf v1]</a>]<font color="#FF0000"> </FONT> </FONT>
</TD></TR>



<TR> <TD class="pubindex"><FONT>[1]&nbsp</FONT></TD> <TD STYLE ="VERTICAL-ALIGN: TOP">
<FONT class="pubtitle">Deeply-Fused Nets. </FONT>
<FONT >Jingdong Wang, Zhen Wei, and Ting Zhang. </FONT>
<FONT class="pubbooktitle">arXiv.</FONT>
<FONT >&nbsp;[<a href="http://arxiv.org/pdf/1605.07716.pdf" target="_blank">pdf</a>]
[<a href="https://github.com/zlmzju/fusenet" target="_blank">code</a>]
 <font color="#FF0000"> </FONT> </FONT>
</TD></TR>
</table>
<br>
</SECTION>

<section id ="bottom" class="sectionwhite">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >
<div class="sectionwhite"><hr></div>
<tr>
<td>
<p class=MsoNormal align=center style='text-align:center'>
<a href="http://www.easycounter.com/">
<img src="http://www.easycounter.com/counter.php?welleast" border="0" alt="HTML Hit Counters"></a>
</p>
</td>
</tr></table>
</body>


</html>


<!--<div style="margin-top:5px">
<iframe src="https://ghbtns.com/github-btn.html?user=leoxiaobin&repo=deep-high-resolution-net.pytorch&type=watch&count=true&v=2" frameborder="0" scrolling="0" width="105px" height="21px"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=leoxiaobin&repo=deep-high-resolution-net.pytorch&type=star&count=true" frameborder="0" scrolling="0" width="102px" height="21px"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=leoxiaobin&repo=deep-high-resolution-net.pytorch&type=fork&count=true" frameborder="0" scrolling="0" width="110px" height="21px"></iframe>
</div>-->
