
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=GB2312">
<meta name="keywords" content="High-Resolution Representation">
<meta name="description" content="Deep High-Resolution Representation Learning">
<!--<link href="CSS/bootstrap.min.css" media="all" rel="stylesheet">-->
<link href="../../CSS/main.css" media="all" rel="stylesheet">
<title>Deep High-Resolution Representation Learning</title>
</head>

<style>
  font.italic {
    font-style: italic;
    }
    .isDisabled {
  pointer-events: none;
  cursor: default;
  text-decoration: none;
  color: black;
}

  .lightfont{font-size: medium; font-weight: normal;}
</style>


<body>


<section id ="headinformation" class="sectionwhite">
<table id="tbInformation" width="100%">
    <tr><td align="center">
    <h1>Deep High-Resolution Representation Learning for Human Pose Estimation </h1>
    </td></tr>
    <TR> <td></td> </TR>
    <TR> <td align="center">Ke Sun &nbsp&nbsp&nbsp Bin Xiao &nbsp&nbsp&nbsp Dong Liu &nbsp&nbsp&nbsp <A  href="../../index.html" target="_blank">Jingdong Wang</A></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <tr><td align="center">
    <img src="images/HRNet.png" border="12" height="170">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
    <img src="images/PoseEstimationQualitativeResults.jpg" border="12" height="170">
    </td></tr>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
</SECTION>





<SECTION id ="Abstract" class="sectionwhite">
<h3> Abstract </h3>
<font size ="normal">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >
    <tr><td align="left">
In this paper, we are interested in
the human pose estimation problem
with a focus on learning reliable high-resolution representations.
Most existing methods
<font class="italic" color="blue">recover high-resolution representations
from low-resolution representations</font>
produced by
a high-to-low resolution network.
Instead,
our proposed network
<font class="italic">maintains high-resolution representations</font>
through the whole process.
</td></tr>
<tr><td align="left">
We start from a high-resolution subnetwork as the first stage,
gradually add high-to-low resolution subnetworks one by one
to form more stages,
and <font class="italic" color="blue"> connect the mutli-resolution subnetworks in parallel</font>.
We conduct <font class="italic" color="blue">repeated multi-scale fusions</font>
such that each of the high-to-low resolution representations
receives information from other parallel representations
over and over,
leading to rich high-resolution representations.
As a result,
the predicted keypoint heatmap
is potentially more accurate and spatially more precise.
We empirically demonstrate the effectiveness
of our network
through the superior pose estimation results
over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset.
In addition, we show the superiority of our network in pose tracking on
the PoseTrack dataset.
</td></tr>
<tr><td align="left">
The code and models are publicly available at <A href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank">GitHub</A>.
    </td></tr>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
</font>
</SECTION>


<SECTION id ="Summary"  class="sectionwhite">
<h3> Summary of existing solutions</h3>
<font size ="normal">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >
<tr><td align="center">
(a) Hourglass [1]; (b) Cascaded pyramid networks [2]; (c) Simple baseline [3]; (d) Combined with dilated convolutions [4].
</td> </TR>
<tr><td align="center">
    <img src="images/Hourglass.png" border="12" width="48%"> &nbsp&nbsp&nbsp <img src="images/CPN.png" border="12" width="48%">
</td> </TR>
<tr><td align="center">
    <img src="images/SimpleBaseline.png" border="12" width="48%"> &nbsp&nbsp&nbsp <img src="images/DilationWupsampleLegendPlus.png" border="12" width="48%">
</td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
<br>
</font>
</SECTION>

<!--<SECTION id ="Results" class="sectionwhite">
<h3> Results</h3></SECTION>
<SECTION id ="Citation" width="80%" class="sectiongray">
<table cellpadding=3 cellspacing=0 width="80%" class="sectiongwhite" >
</table>
</SECTION>-->


<SECTION id ="Paper"  class="sectionwhite">
<h3> Paper</h3>
<font size ="normal">
<table cellpadding=0 cellspacing=0 width="100%" class="sectionwhite">
<tr><td align="center">
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper001.jpg" style="border:1px solid black" width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper002.jpg" style="border:1px solid black"  width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper003.jpg" style="border:1px solid black"  width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper004.jpg" style="border:1px solid black"  width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper005.jpg" style="border:1px solid black"  width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    <img src="images/PoseEstimation/paper006.jpg" style="border:1px solid black"  width="15.5%"></A>
    <A href="https://arxiv.org/pdf/1902.09212.pdf" target="_blank">
    paper</A>
</td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
</font>
</SECTION>





<SECTION id ="Code"  class="sectionwhite">
<h3> Code</h3>
<font size ="normal">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite">
<tr><td> 
We released the training and testing code and the pretrained model at <A href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank">GitHub</A> </td></td>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
</font>
</SECTION>

<SECTION id ="Applications" class="sectionwhite">
<h3> Other applications <font class ="lightfont"> (coming soon) </font></h3> 
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >
    <tr>
    <td align ="center"><A href="PoseEstimation.html" target="_blank"><img src="images/PoseEstimationExample.jpg" border="12" height="100"></A></td>
    <td align ="center"><A class ="isDisabled" href="SemanticSegmentation.html" target="_blank"><img src="images/ImageSegmentationExample.jpg" border="12" height="100"></A></A></td>
    <td align ="center"><A class ="isDisabled" href="FaceAlignment.html" target="_blank"><img src="images/FaceAlignmentExample.jpg" border="12" height="100"></A></td>
    <td align ="center"><A class ="isDisabled" href="ImageClassification.html" target="_blank"><img src="images/ImageClassificationExample.jpg" border="12" height="100"></A></td>
    <td align ="center"><A class ="isDisabled" href="ObjectDetection.html" target="_blank"><img src="images/ObjectDetectionExample.jpg" border="12" height="100"></A></td>
    <td rowspan="2"> more ... </td
    </tr>

    <tr>
    <td align ="center"><A href="PoseEstimation.html" target="_blank">Pose estimation</A></td>
    <td align ="center"><A class ="isDisabled" href="SemanticSegmentation.html" target="_blank">Semantic segmentation</A></td>    
    <td align ="center"><A class ="isDisabled" href="FaceAlignment.html" target="_blank">Face alignment</A></td>
    <td align ="center"><A class ="isDisabled" href="ImageClassification.html" target="_blank">Image classification</A></td>
    <td align ="center"><A class ="isDisabled" href="ObjectDetection.html" target="_blank">Object detection</A></td>
    </tr>
        <TR> <td></td> </TR>
    <TR> <td></td> </TR>
    <TR> <td></td> </TR>
</table>
</SECTION>

<SECTION id ="Citation" class="sectionwhite">
<h3> Citation</h3></SECTION>
<SECTION id ="Citation" width="60%" class="sectiongray">
<table cellpadding=3 cellspacing=0 width="60%" class="sectiongray" >
<TR><TD>
<pre><code>@inproceedings{SunXLWang2019,
  title={Deep High-Resolution Representation Learning for Human Pose Estimation},
  author={Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang},
  booktitle={CVPR},
  year={2019}
}
</TD></TR>
</code></pre>
</table>
</SECTION>

<SECTION id ="References" class="sectionwhite">
<h3> References</h3></SECTION>
<SECTION id ="References" class="sectionwhite">
<table cellpadding=3 cellspacing=0 width="100%" class="sectionwhite" >
<TR> <TD class="pubindex"><FONT>[1]&nbsp</FONT></TD>  
<TD STYLE ="VERTICAL-ALIGN: TOP"> A. Newell, K. Yang, and J. Deng. Stacked hourglass net-works for human pose estimation. ECCV, pages 483–499, 2016.</TD></TR>

<TR> <TD class="pubindex"><FONT>[2]&nbsp</FONT></TD>
<TD STYLE ="VERTICAL-ALIGN: TOP"> Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun. Cascaded pyramid network for multi-person pose estimation. CoRR,
    abs/1711.07319, 2017. </TD></TR>

<TR> <TD class="pubindex"><FONT>[3]&nbsp</FONT></TD>
<TD STYLE ="VERTICAL-ALIGN: TOP"> B. Xiao, H. Wu, and Y. Wei. Simple baselines for human pose estimation and tracking. ECCV, pages 472–487, 2018. </TD></TR>
    
<TR> <TD class="pubindex"><FONT>[4]&nbsp</FONT></TD>
<TD STYLE ="VERTICAL-ALIGN: TOP"> E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person
    pose estimation model. ECCV, pages 34–50, 2016. </TD></TR>
</table>
</SECTION>

 


<div><hr></div>

<tr>
<td>
<p class=MsoNormal align=center style='text-align:center'>
<a href="http://www.easycounter.com/">
<img src="http://www.easycounter.com/counter.php?welleast" border="0" alt="HTML Hit Counters"></a>
</p>
</td>
</tr>
</body>


</html>
