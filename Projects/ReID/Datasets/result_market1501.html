<html>
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="js/jquery.min.js"></script>
    <script src="js/common.js"></script>

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
  </head>

  <body>
    <div id="nav-jumbo-placeholder"> </div>
    <div class="jumbotron">
            <div class="container">
            <div class="text-center">
                    <h1 style=color:aliceblue>Market-1501</h1>
                    <p style=color:aliceblue>The leaderboard</p>
                <!-- <p><a class="btn btn-primary btn-lg" href="registration.html" role="button">Register Now</a></p> -->
            </div>
            </div>
        </div>
    <div style="width: 1200px; margin:0 auto;">
      <h3>Baseline</h3>
    	<table id="fmeasure" class="table table-striped table-bordered" cellspacing="0" width="100%">
        <thead>
            <tr>
                <th style=min-width:300px>Paper Name</th>
                <th style=min-width:80px>rank-1</th>
                <th style=min-width:80px>rank-5</th>
                <th style=min-width:80px>rank-10</th>
                <th style=min-width:80px>rank-20</th>
                <th style=min-width:80px>rank-30</th>
                <th style=min-width:80px>rank-50</th>
                <th style=min-width:80px>mAP</th>
                <th style=min-width:300px>Notes</th>
            </tr>
        </thead>
        <tbody>
              <td rowspan="8">Scalable person re-identification: a benchmark<a href="#ref">[1]</a></td> <td>8.28</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2.23</td> <td rowspan="1">gBiCov<a href="#ref44">[47]</a>, Euclidean distance, single query</td>
              </tr>

              <tr>
                  <td>9.62</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2.72</td><td rowspan="1">HistLBP<a href="#ref45">[48]</a>, Euclidean distance, single query. Super thanks to Mengran Gou for sending us the evaluation results</td>
              </tr>
              <tr>
                  <td>26.07</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>7.75</td><td rowspan="1">LOMO<a href="#ref46">[49]</a>, Euclidean distance, single query</td>
              </tr>
              <tr>
                  <td>35.84</td><td>52.40</td><td>60.33</td><td>67.64</td><td>71.88</td><td>75.80</td><td>14.75</td><td rowspan="1">BoW, Euclidean distance, single query</td>
              </tr>
              <tr>
                  <td>44.36</td><td>60.24</td><td>66.48</td><td>73.25</td><td>76.19</td><td>79.69</td><td>19.42</td><td rowspan="1">BoW, Euclidean distance, multiple query</td>
              </tr>
              <tr>
                  <td>34.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>15.66</td><td rowspan="1">BoW + LMNN, single query</td>
              </tr>
              <tr>
                  <td>38.21</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>17.05</td><td rowspan="1">BoW + ITML, single query</td>
              </tr>
              <tr>
                  <td>44.42</td><td>63.90</td><td>72.18</td><td>78.95</td><td>82.51</td><td>87.05</td><td>20.76</td><td rowspan="1">BoW + KISSME, single query</td>
              </tr>

              <tr>
              <td rowspan="2">Person re-identification: Past, Present and Future<a href="#ref">[2]</a></td> <td>55.49</td><td>76.28</td><td>83.55</td><td>88.98</td><td>91.72</td><td>93.97</td><td>32.36</td> <td rowspan="1">AlexNet identification model, using FC7 (4,096-dim) and Euclidean distance for testing, single query. This method is also used in<a href="#ref47">[50, 51]</td>
              </tr>
              <tr>
                  <td>73.90</td><td>87.68</td><td>91.54</td><td>94.80</td><td>96.02</td><td>97.21</td><td>47.78</td><td rowspan="1">ResNet-50 identification model, using Pool5 (2,048-dim) and Euclidean distance for testing, single query</td>
              </tr>
        </tbody>
      </table>
      <h3>Results of supervised approaches</h3>
      <table id="fmeasure" class="table table-striped table-bordered" cellspacing="0" width="100%">
        <thead>
            <tr>
                <th style=min-width:300px>Paper Name</th>
                <th style=min-width:80px>rank-1</th>
                <th style=min-width:80px>rank-5</th>
                <th style=min-width:80px>rank-10</th>
                <th style=min-width:80px>rank-20</th>
                <th style=min-width:80px>rank-30</th>
                <th style=min-width:80px>rank-50</th>
                <th style=min-width:80px>mAP</th>
                <th style=min-width:300px>Notes</th>
            </tr>
          </thead>
          <tbody><tr>
              <td rowspan="1">Multiregion Bilinear Convolutional Neural Networks for Person Re-Identification<a href="#ref">[3]</a></td>.</td> <td>66.36</td> <td>85.01</td> <td>90.17</td><td>-</td><td>-</td><td>-</td><td>41.17</td><td>Multiregion Bilinear DML, single query. </td>
              </tr>
              <tr>
              <td>Scalable Metric Learning via Weighted Approximate Rank Component Analysis<a href="#ref1">[4]</a></td> <td>45.16</td> <td>68.12</td> <td>76</td><td>84</td><td>87</td><td>-</td><td>-</td> <td>Use the baseline BoW descriptor and the proposed WARCA metric learning method.</td>
              </tr>
              <tr>
              <td>A Comprehensive Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets<a href="#ref2">[5]</a></td> <td>46.5</td> <td>71.1</td> <td>79.9</td><td>86.9</td><td>-</td><td>-</td><td>-</td> <td>HistLBP+kLFDA. Single query.</td>
              </tr>
              <tr>
              <td>Temporal Model Adaptation for Person Re-Identification<a href="#ref3">[6]</a></td> <td>47.92</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>22.31</td> <td>Using 13.58% of the labeled data. Single query.</td>
              </tr>
              <tr>
              <td>Deep Linear Discriminant Analysis on Fisher Networks: A Hybrid Architecture for Person Re-identification<a href="#ref4">[7]</a></td> <td>48.15</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>29.94</td> <td>Combines Fisher vector and deep neural network. Not sure whether multiple queries are used.</td>
              </tr>
              <tr>
              <td rowspan="2">Learning a Discriminative Null Space for Person Re-identification<a href="#ref5">[8]</a></td> <td>55.43</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>29.87</td><td>LOMO+Discriminative Null Space, single query. </td>
              </tr>
              <tr>
                  <td>71.56</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>46.03</td><td>Both multiple query (MQ) and score-level feature fusion are used.</td>
              </tr>
            <tr>
                <td>Similarity Learning with Spatial Constraints for Person Re-identification<a href="#ref6">[9]</a></td> <td>51.90</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>26.35</td> <td>Extract HSV, LAB, HOG, and SILTP features from patches, and use the proposed SCSP method. Single query.</td>
            </tr>
              <tr>
              <td>PersonNet: Person Re-identification with Deep Convolutional Neural Networks<a href="#ref7">[10]</a></td> <td>37.21</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>18.57</td><td>Use single query. Similarity between boxes is learnt end-to-end through a deep network. </td>
              </tr>
              <tr>
              <td>End-to-End Comparative Attention Networks for Person Re-identification<a href="#ref8">[11]</a></td> <td>48.24</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>24.43</td><td>Use single query. Features are learned by the Comparative Attention Network </td>
              </tr>
              <tr>
              <td rowspan="2">Deep Attributes Driven Multi-Camera Person Re-identification<a href="#ref9">[12]</a></td> <td>39.4</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>19.6</td><td>single query. </td>
              </tr>
                <tr>
                  <td>49.0</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>25.8</td><td>Multiple query.</td>
              </tr>
              <tr>
              <td rowspan="2">Multi-Scale Triplet CNN for Person Re-Identification<a href="#ref10">[13]</a></td> <td>45.1</td> <td>70.1</td> <td>78.4</td><td>-</td><td>88.7</td><td>-</td><td>-</td><td>single query. Use a triplet loss CNN model with multi-scale improvement. </td>
              </tr>
              <tr>
                  <td>55.4</td> <td>78.9</td> <td>85.6</td><td>-</td><td>93.7</td><td>-</td><td>-</td><td>Multiple query</td>
              </tr>
              <tr>
              <td>Learning Deep Embeddings with Histogram Loss<a href="#ref11">[14]</a></td><td> <td>59.47</td> <td>80.73</td> <td>86.94</td><td>91.09</td><td>-
          </td><td>-</td><td>It seems the single query mode is chosen. A previously introduced deep metric learning framework is adopted, but with new loss functions.  </td>
              </tr>
              <tr>
              <td>A Siamese Long Short-Term Memory Architecture for Human Re-Identification<a href="#ref12">[15]</a></td> <td>61.6</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>35.3</td><td>Use multiple queries. The LSTM model processes image regions sequentially. </td>
              </tr>
            <tr>
              <td rowspan="2">Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification<a href="#ref13">[16]</a></td> <td>65.88</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>39.55</td><td>single query. Feature learned by the Gated Siamese CNN.</td>
              </tr>

              <tr>
                  <td>76.04</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>48.45</td><td>Multiple query</td>
              </tr>

            <tr>
              <td rowspan="2">Point to Set Similarity Based Deep Feature Learning for Person Re-identification<a href="#ref14">[17]</a></td> <td>70.72</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>44.27</td><td>single query. The pairwise loss, triplet loss and a regularizor are jointly optimzed in the loss function.</td>
              </tr>

              <tr>
                  <td>85.78</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>55.73</td><td>Multiple query</td>
              </tr>


            <tr>
              <td rowspan="2">Person Re-Identification by Camera Correlation Aware Feature Augmentation<a href="#ref15">[18]</a></td> <td>71.8</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>45.5</td><td>single query. Use CRAFT-MFA+LOMO</td>
              </tr>
              <tr>
                  <td>79.7</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>54.3</td><td>Multiple query</td>
              </tr>

              <tr>
              <td rowspan="2">Consistent-Aware Deep Learning for Person Re-identification in a Camera Network<a href="#ref16">[19]</a></td> <td>73.84</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>47.11</td><td>single query. Pairwise similarities are considered across multiple cameras for samples in a batch.</td>
              </tr>
              <tr>
                  <td>80.85</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>55.58</td><td>Multiple query</td>
              </tr>	

              <tr>
              <td rowspan="2">Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in Re-identification<a href="#ref17">[20]</a></td> <td>73.87</td> <td>88.03</td> <td>92.22</td><td>95.07</td><td>96.20</td><td>97.39</td><td>47.89</td><td>single query. Use SOMAnet and Market1501 as training set.</td>
              </tr>
              <tr>
                  <td>81.29</td> <td>92.61</td> <td>95.31</td><td>97.12</td><td>97.68</td><td>98.43</td><td>56.98</td><td>Multiple query</td>
              </tr>

              <tr>
              <td rowspan="1">Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion<a href="#ref18">[21]</a></td> <td>76.9</td> <td>91.5</td> <td>94.6</td><td>96.7</td><td>-</td><td>-</td><td>-</td><td>single query. CPM is trained on MPII for pose estimation and part localization.</td>
              </tr>

              <tr>
              <td>Re-ranking Person Re-identification with k-reciprocal Encoding<a href="#ref19">[22]</a></td> <td>77.11</td> <td>-</td> <td>-</td><td>-</td><td>-
          </td><td>-</td><td>63.63</td><td>Single query. Re-ranking is performed. </td>
              </tr>

              <tr>
              <td>Pose Invariant Embedding for Deep Person Re-identification<a href="#ref20">[23]</a></td> <td>79.33</td> <td>90.76</td> <td>94.41</td><td>96.52</td><td>-
          </td><td>-</td><td>55.95</td><td>Single query. The PIE descriptor and kissme is used. </td>
              </tr>

            <tr>
              <td rowspan="2">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro<a href="#ref21">[24]</a></td> <td>78.06</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>56.23</td><td>single query. GAN images are used in the ResNet baseline.</td>
              </tr>
              <tr>
                  <td>85.12</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>68.52</td><td>Multiple query</td>
              </tr>

            <tr>
              <td rowspan="2">A Discriminatively Learned CNN Embedding for Person Re-identification<a href="#ref22">[25]</a></td> <td>79.51</td> <td>90.91</td> <td>94.09</td><td>96.23</td><td>97.33</td><td>98.25</td><td>59.87</td><td>single query. Identification and Verification losses are used in a siamese network based on ResNet-50.</td>
              </tr>
              <tr>
                  <td>85.84</td> <td>94.54</td> <td>96.41</td><td>97.51</td><td>98.07</td><td>98.81</td><td>70.33</td><td>Multiple query</td>
              </tr>


            <tr>
              <td rowspan="2">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification<a href="#ref23">[26]</a></td> <td>80.31</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>57.53</td><td>single query. Latent body parts are discovered by the spatial transformer network instead of rigid partitioning.</td>
              </tr>
              <tr>
                  <td>86.79</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>66.70</td><td>Multiple query</td>
              </tr>


            <tr>
              <td rowspan="1">Deeply-Learned Part-Aligned Representations for Person Re-Identification<a href="#ref24">[27]</a></td> <td>81.0</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>63.4</td><td>single query.  Body parts are detected from feature maps and their respective features are concatenated later.</td>
              </tr>

            <tr>
              <td rowspan="2">Scalable Person Re-identification on Supervised Smoothed Manifold<a href="#ref25">[28]</a></td> <td>82.21</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>68.80</td><td>single query. IDE+re-ranking.</td>
              </tr>
              <tr>
                  <td>88.18</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>76.18</td><td>Multiple query</td>
              </tr>	

            <tr>
              <td rowspan="1">Divide and Fuse: A Re-ranking Approach for Person Re-identification<a href="#ref26">[29]</a></td> <td>82.30</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>72.42</td><td>single query. Features are divided into sub-vectors before re-encoded into a new vector. The new vectors are fused into one vector for ranking.</td>
              </tr>

              <tr>
              <td>SVDNet for Pedestrian Retrieval<a href="#ref27">[30]</a></td> <td>82.3</td> <td>-</td> <td>-</td><td>-</td><td>-
          </td><td>-</td><td>62.1</td><td>Single query. 1,024-dim pool5 feature from svdnet is used. </td>
              </tr>

              <tr>
              <td>Pose-driven Deep Convolutional Model for Person Re-identification<a href="#ref28">[31]</a></td> <td>84.14</td> <td>92.73</td> <td>94.92</td><td>96.82</td><td>-</td><td>-</td><td>63.41</td><td>Single query. Human part is discovered with pose models. Local and Global images are used for feature learning.</td>
              </tr>

            <tr>
              <td rowspan="2">Deep Transfer Learning for Person Re-identification<a href="#ref29">[32]</a></td> <td>83.7</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>65.5</td><td>single query. Identification and Verification losses are used in a siamese network based on GoogleNet.</td>
              </tr>
              <tr>
                  <td>89.6</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>73.8</td><td>Multiple query</td>
              </tr>	

              <tr><td>Improving Person Re-identification by Attribute and Identity Learning<a href="#ref30">[33]</a></td> <td>84.29</td><td>93.20</td> <td>95.19</td><td>97.00</td><td>-</td><td>-</td><td>64.67</td><td>Single query. Attributes and ID classification are jointly learning. </td>
              </tr>

            <tr>
              <td rowspan="4">Pedestrian Alignment Network for Person Re-identification<a href="#ref31">[34]</a></td> <td>82.81</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>63.35</td><td>single query. Pedestrians are aligned by the Spatial Transformer Network. Results could be higher when fine-tuning on the GAN model<a href="#ref21">[24]</a>.</td>
              </tr>
              <tr>
                  <td>85.78</td> <td>93.38</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>76.56</td><td>Single query + re-ranking<a href="#ref19">[22]</a></td>
              </tr>
              <tr>
                  <td>88.18</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>71.72</td><td>Multiple query</td>
              </tr>
              <tr>
                  <td>89.79</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>83.79</td><td>Multiple query + re-ranking<a href="#ref19">[22]</td>
              </tr>

            <tr>
              <td rowspan="4">Person re-identification by deep joint learning of multi-loss classification<a href="#ref32">[35]</a></td> <td>83.9</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>64.4</td><td>single query. Stripes and global images are jointly considered in a classification CNN network with multiple streams.</td>
              </tr>
              <tr>
                  <td>88.8</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>72.9</td><td>Single query + re-ranking<a href="#ref19">[22]</td>
              </tr>
              <tr>
                  <td>85.1</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>65.5</td><td>single query, 4 body parts</td>
              </tr>
              <tr>
                  <td>89.7</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>74.5</td><td>Multiple query, 4 body parts</td>
              </tr>

            <tr>
              <td rowspan="4">In Defense of the Triplet Loss for Person Re-Identification<a href="#ref33">[36]</a></td> <td>84.92</td> <td>94.21</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>69.14</td><td>single query. The triplet-loss based network is fine-tuned. Image size: 256x128. The last layer in ResNet is replaced with one 1,024-dim layer and one 128-dim layer. Batch normalization is used as well.</td>
              </tr>
              <tr>
                  <td>86.67</td> <td>93.38</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>81.07</td><td>Single query + re-ranking<a href="#ref19">[22]</td>
              </tr>
              <tr>
                  <td>90.53</td> <td>96.29</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>76.42</td><td>Multiple query</td>
              </tr>
              <tr>
                  <td>91.75</td> <td>95.78</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>87.18</td><td>Multiple query + re-ranking<a href="#ref19">[22]</td>
              </tr>

            <tr>
              <td rowspan="2">Dual Mutual Learning<a href="#ref34">[37]</a></td> <td>87.73</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>68.83</td><td>single query. Two MoblieNets learn from each other, and the average re-ID results of the two individual networks is reported.</td>
              </tr>
              <tr>
                  <td>91.66</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>77.14</td><td>multiple query</td>
              </tr>

            <tr>
              <td rowspan="2">Random Erasing Data Augmentation<a href="#ref35">[38]</a></td> <td>87.08</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>71.31</td><td>single query. SVDNet + random erasing data augmentation.</td>
              </tr>
              <tr>
                  <td>89.13</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>83.93</td><td>Re-ranking is used on the rank list obtained by single query</td>
              </tr>



          </tbody>
        </table>
        <h3>Results of  unsupervised/transfer learning</h3>
        <table id="fmeasure" class="table table-striped table-bordered" cellspacing="0" width="100%">
            <thead>
              <tr>
                  <th style=min-width:300px>Paper Name</th>
                  <th style=min-width:80px>rank-1</th>
                  <th style=min-width:80px>rank-5</th>
                  <th style=min-width:80px>rank-10</th>
                  <th style=min-width:80px>rank-20</th>
                  <th style=min-width:80px>rank-30</th>
                  <th style=min-width:80px>rank-50</th>
                  <th style=min-width:80px>mAP</th>
                  <th style=min-width:300px>Notes</th>
              </tr>
            </thead>
          <tbody>
            <tr>
              <td rowspan="2">Efficient Online Local Metric Adaptation via Negative Samples for Person Re-Identification<a href="#ref36">[39]</a></td> <td>40.93</td> <td>-</td> <td>-</td><td>74.06</td><td>-</td><td>-</td><td>-</td><td>Single query. LOMO is used for initialization. This method does not need any positive pairs.</td>
              </tr>
              <tr>
                  <td>51.45</td> <td>-</td> <td>-</td><td>80.98</td><td>-</td><td>-</td><td>-</td><td>Multiple query. </td>
              </tr>
            <tr>
                <td>Unsupervised Person Re-identification: Clustering and Fine-tuning<a href="#ref37">[40]</a></td><td>41.9</td> <td>57.3</td> <td>64.3</td><td>70.5</td><td>-</td><td>-</td><td>18.0</td><td>Single query. An IDE model trained on CUHK03 is used for initialization. </td>
          </tr>

          <tr>
          <td rowspan="1">Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification<a href="#ref38">[41]</a></td> <td>54.5</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>26.3</td><td>Multiple query. JSTL is used for initialization. A clustering method is used for label estimation.</td>
          </tr>

          <tr>
          <td rowspan="3">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification<a href="#ref39">[42]</a></td> <td>51.5</td> <td>70.1</td> <td>76.8</td><td>-</td><td>-</td><td>-</td><td>22.8</td><td>Single query. DukeMTMC<a href="#ref21">[24]</a> labels are used for domain adaptation. SPGAN is an improved version of CycleGAN.</td>
          </tr>
          <tr>
              <td>57.7</td> <td>75.8</td> <td>82.4</td><td>-</td><td>-</td><td>-</td><td>26.7</td><td>Single query. Local max pooling is used in addition to SPGAN.</td>
          </tr>
          <tr>
              <td>57.0</td> <td>73.9</td> <td>80.3</td><td>-</td><td>-</td><td>-</td><td>27.1</td><td>Multiple query. SPGAN is used without local max pooling.</td>
          </tr>

          </tbody>
        </table>
        <h3>Use the dataset, but do not report results/use different evaluation protocols</h3>
        <table id="fmeasure" class="table table-striped table-bordered" cellspacing="0" width="100%">
          <thead>
            <tr>
                <th style=min-width:300px>Paper Name</th>
                <th style=min-width:80px>rank-1</th>
                <th style=min-width:80px>rank-5</th>
                <th style=min-width:80px>rank-10</th>
                <th style=min-width:80px>rank-20</th>
                <th style=min-width:80px>rank-30</th>
                <th style=min-width:80px>rank-50</th>
                <th style=min-width:80px>mAP</th>
                <th style=min-width:300px>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Constrained Deep Metric Learning for Person Re-identification<a href="#ref40">[43]</a></td> <td>-</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Used together with CUHK03 as training data for the proposed Constrained Deep Metric Learning. Test on CUHK01 and VIPeR. </td>
              </tr>
              <tr>
              <td>An Enhanced Deep Feature Representation for Person Re-identification<a href="#ref41">[44]</a></td> <td>-</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Used as training data for the proposed Feature Fusion Net. Testing is performed on other benchmarks. </td>
              </tr>
              <tr>
              <td>Semantics-Aware Deep Correspondence Structure Learning for Robust Person Re-identification<a href="#ref42">[45]</a></td> <td>-</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Used as training data for the proposed DCSL model. </td>
              </tr>
            <tr>
              <td rowspan="2">Human-In-The-Loop Person Re-Identification<a href="#ref43">[46]</a></td> <td>78.0</td> <td>-</td> <td>-</td><td>-</td><td>-</td><td>86.0</td><td>-</td><td>1000 identities, 300 queries are used. Single Shot. 6 random splits.</td>
              </tr>
              <tr>
                  <td>33.8</td> <td>61.0</td> <td>73.6</td><td>85.3</td><td>-</td><td>-</td><td>-</td><td> 501 identities, single shot, 6 random splits. We assume 501 queries are used.</td>
              </tr>
          </tbody>
        </table>
        
        <h3>Market-1501+500k Leaderboard</h3>
        <table id="500kstateofart" class="table table-striped table-bordered" cellspacing="0" width="100%">
            <tbody>
                <thead>
                  <tr>
                      <th rowspan="2" style=min-width:300px>Paper Name</th>
                      <th colspan="5" style=min-width:560px>Gallery size</th>
                      <th rowspan="2" style=min-width:300px>Notes</th>
                  </tr>
                  <tr>
                      <th style=min-width:112px>metric</th>
                      <th style=min-width:112px>19,732</th>
                      <th style=min-width:112px>119,732</th>
                      <th style=min-width:112px>219,732</th>
                      <th style=min-width:112px>519,732</th>
                  </tr>
                </thead>
                <!-- <tr>
            <td width="35%" rowspan="2"><strong>Paper Name</strong></td> <td width="30%" style="text-align: center" colspan="5"><strong>Gallery size</strong></td> <td width="30%" style="text-align: center" rowspan="2"><strong>Notes</strong></td>
            </tr>
            <tr>
                <td>metric</td><td>19,732</td><td>119,732</td><td>219,732</td><td>519,732</td>
            </tr> -->
            <tr>
            <td rowspan="4">Scalable person re-identification: a benchmark<a href="#ref">[1]</a></td> <td>mAP</td><td>13.94</td><td>11.44</td><td>10.52</td><td>8.66</td><td rowspan="1">BoW, Euclidean distance, single query</td>
            </tr>

            <tr>
                <td>mAP</td><td>13.85</td><td>10.88</td><td>9.75</td><td>7.56</td><td rowspan="1">BoW+ANN<a href="#ref49">[52]</a>, single query</td>
            </tr>
            <tr>
                <td>mAP</td><td>18.38</td><td>15.95</td><td>14.88</td><td>12.60</td><td rowspan="1">BoW, Euclidean distance, multiple query</td>
            </tr>
            <tr>
                <td>mAP</td><td>18.26</td><td>15.09</td><td>13.75</td><td>10.92</td><td rowspan="1">BoW+ANN<a href="#ref49">[52]</a>, multiple query</td>
            </tr>


            <tr>
            <td rowspan="2">Person re-identification: Past, Present and Future<a href="#ref">[2]</a></td> <td>rank-1</td><td>73.69</td><td>72.15</td><td>71.55</td><td>70.67</td> <td rowspan="2">ResNet50 baseline. The 2,048-dim feature from pool5 is used under Euclidean distance. Code can be accessed <a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501"><font color="blue">here</font></a>.</td>
            </tr>
            <tr>
                <td>mAP</td><td>51.48</td><td>48.72</td><td>47.57</td><td>46.05</td>
            </tr>

        <!--     <tr>
            <td>"Scalable Metric Learning via Weighted Approximate Rank Component Analysis", Cijo Jose, Fran&ccedil;ois Fleuret, ECCV 2016</td> <td>45.16</td> <td>68.12</td> <td>76</td><td>84</td><td>87</td> <td>Use the baseline BoW descriptor and the proposed WARCA metric learning method.</td>
            </tr> -->


          </tbody>
        </table>
        <h3>More results</h3>
        <table id="500kstateofart2" class="table table-striped table-bordered" cellspacing="0" width="100%">
            <tbody>
                <thead>
                  <tr>
                      <th rowspan="2" style=min-width:300px>Paper Name</th>
                      <th colspan="5" style=min-width:560px>Gallery size</th>
                      <th rowspan="2" style=min-width:300px>Notes</th>
                  </tr>
                  <tr>
                      <th style=min-width:112px>metric</th>
                      <th style=min-width:112px>19,732</th>
                      <th style=min-width:112px>119,732</th>
                      <th style=min-width:112px>219,732</th>
                      <th style=min-width:112px>519,732</th>
                  </tr>
                </thead>


            <tr>
            <td rowspan="2">A Discriminatively Learned CNN Embedding for Person Re-identification<a href="#ref22">[25]</a></td> <td>rank-1</td> <td>79.51</td> <td>73.78</td><td>71.50</td><td>68.26</td><td rowspan="2">A two-stream network based on ResNet50, single query. Code is available upon request.</td>
            </tr>
            <tr>
                <td>mAP</td> <td>59.87</td> <td>52.28</td><td>49.11</td><td>45.24</td>
            </tr>	

            <tr>
            <td rowspan="2">Improving Person Re-identification by Attribute and Identity Learning<a href="#ref30">[33]</a></td> <td>rank-1</td> <td>83.99</td> <td>79.89</td><td>78.20</td><td>75.44</td><td rowspan="2">Attribute and ID classification is jointly learned. ResNet50 is used as backbone. Pool5 feature is used under Euclidean distance, single query. Attribute labels can be accessed <a href="https://vana77.github.io/"><font color="blue">here</font></a>.</td>
            </tr>
            <tr>
                <td>mAP</td> <td>62.83</td> <td>56.46</td><td>53.58</td><td>49.78</td>
            </tr>


            <tr>
            <td rowspan="2">In Defense of the Triplet Loss for Person Re-Identification<a href="#ref33">[36]</a></td> <td>rank-1</td> <td>84.92</td> <td>79.69</td><td>77.88</td><td>74.70</td><td rowspan="2">single query. The triplet-loss based network is fine-tuned. Image size: 256x128. The last layer in ResNet is replaced with one 1,024-dim layer and one 128-dim layer. Batch normalization.</td>
            </tr>
            <tr>
                <td>mAP</td> <td>69.14</td> <td>61.93</td><td>58.74</td><td>53.63</td>
            </tr>
        <!--     <tr>
            <td>"Scalable Metric Learning via Weighted Approximate Rank Component Analysis", Cijo Jose, Fran&ccedil;ois Fleuret, ECCV 2016</td> <td>45.16</td> <td>68.12</td> <td>76</td><td>84</td><td>87</td> <td>Use the baseline BoW descriptor and the proposed WARCA metric learning method.</td>
            </tr> -->
        </tbody></table>


      
        <h2 id="ref">References</h2>
        <ul style="list-style-type:none">
            <li id="ref1">[1] "Scalable person re-identification: a benchmark", Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, Qi Tian, ICCV 2015.</li>
            <li id="ref2">[2] "Person re-identification: Past, Present and Future", Liang Zheng, Yi Yang, Alexander Hauptmann, arXiv 2016.</li>
            <li id="ref3">[3] "Multiregion Bilinear Convolutional Neural Networks for Person Re-Identification", Evgeniya Ustinova, Yaroslav Ganin, Victor Lempitsky, AVSS 2017.</li>
            <li id="ref4">[4] "Scalable Metric Learning via Weighted Approximate Rank Component Analysis", Cijo Jose, Fran√ßois Fleuret, ECCV 2016.</li>
            <li id="ref5">[5] "A Comprehensive Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets", Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels Rates-Borras, Octavia Camps, Richard J. Radke, ArXiv 2016.</li>
            <li id="ref6">[6] "Temporal Model Adaptation for Person Re-Identification", Niki Martinel, Abir Das, Christian Micheloni, Amit K. Roy-Chowdhury, ECCV 2016.</li>
            <li id="ref7">[7] "Deep Linear Discriminant Analysis on Fisher Networks: A Hybrid Architecture for Person Re-identification", Lin Wu, Chunhua Shen, Anton van den Hengel, ArXiv 2016.</li>
            <li id="ref8">[8] "Learning a Discriminative Null Space for Person Re-identification", Li Zhang, Tao Xiang, Shaogang Gong, CVPR 2016.</li>
            <li id="ref9">[9] "Similarity Learning with Spatial Constraints for Person Re-identification", Dapeng Chen, Zejian Yuan, Badong Chen, Nanning Zheng, CVPR 2016.</li>
            <li id="ref10">[10] "PersonNet: Person Re-identification with Deep Convolutional Neural Networks", Lin Wu, Chunhua Shen, Anton van den Hengel, ArXiv 2016.</li>
            <li id="ref11">[11] "End-to-End Comparative Attention Networks for Person Re-identification", Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, Shuicheng Yan, ArXiv 2016.</li>
            <li id="ref12">[12] "Deep Attributes Driven Multi-Camera Person Re-identification", Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian, ECCV 2016.</li>
            <li id="ref13">[13] "Multi-Scale Triplet CNN for Person Re-Identification", Jiawei Liu, Zheng-Jun Zha, Qi Tian, Dong Liu, Ting Yao, Qiang Ling, Tao Mei, A 2016.</li>
            <li id="ref14">[14] "Learning Deep Embeddings with Histogram Loss", Evgeniya Ustinova and Victor Lempitsky, NIPS 2016.</li>
            <li id="ref15">[15] "A Siamese Long Short-Term Memory Architecture for Human Re-Identification", Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, Gang Wang, ECCV 2016.</li>
            <li id="ref16">[16] "Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification", Rahul Rama Varior, Mrinal Haloi, Gang Wang, ECCV 2016.</li>
            <li id="ref17">[17] "Point to Set Similarity Based Deep Feature Learning for Person Re-identification", Sanping Zhou, Jinjun Wang, Jiayun Wang, Yihong Gong, Nanning Zheng, CVPR 2017.</li>
            <li id="ref18">[18] "Person Re-Identification by Camera Correlation Aware Feature Augmentation", Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, Jian-Huang Lai, TPAMI 2017.</li>
            <li id="ref19">[19] "Consistent-Aware Deep Learning for Person Re-identification in a Camera Network, Ji Lin, Liangliang Ren, Jiwen Lu, Jianjiang Feng, Jie Zhou, CVPR 2017.</li>
            <li id="ref20">[20] "Looking Beyond Appearances: Synthetic Training Data for Deep CNNs in Re-identification", Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen and Theoharis Theoharis, Arxiv 2017.</li>
            <li id="ref21">[21] "Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion", Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang, CVPR 2017.</li>
            <li id="ref22">[22] "Re-ranking Person Re-identification with k-reciprocal Encoding", Zhun Zhong, Liang Zheng, Donglin Cao and Shaozi Li, CVPR 2017.</li>
            <li id="ref23">[23] "Pose Invariant Embedding for Deep Person Re-identification", Liang Zheng, Yujia Huang, Huchuan Lu, and Yi Yang, Arxiv 2017.</li>
            <li id="ref24">[24] "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro", Zhedong Zheng, Liang Zheng, Yi Yang, ICCV 2017.</li>
            <li id="ref25">[25] "A Discriminatively Learned CNN Embedding for Person Re-identification", Zhedong Zheng, Liang Zheng, Yi Yang, Arxiv 2016.</li>
            <li id="ref26">[26] "Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification", Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang, CVPR 2017.</li>
            <li id="ref27">[27] "Deeply-Learned Part-Aligned Representations for Person Re-Identification", Liming Zhao, Xi Li, Jingdong Wang, Yueting Zhuang, ICCV 2017.</li>
            <li id="ref28">[28] "Scalable Person Re-identification on Supervised Smoothed Manifold", Song Bai, Xiang Bai, Qi Tian, CVPR 2017.</li>
            <li id="ref29">[29] "Divide and Fuse: A Re-ranking Approach for Person Re-identification", Rui Yu, Zhichao Zhou, Song Bai, Xiang Bai, BMVC 2017.</li>
            <li id="ref30">[30] "SVDNet for Pedestrian Retrieval", Yifan Sun, Liang Zheng, Weijian Deng, Shengjin Wang, ICCV 2017.</li>
            <li id="ref31">[31] "Pose-driven Deep Convolutional Model for Person Re-identification", Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian, ICCV 2017.</li>
            <li id="ref32">[32] "Deep Transfer Learning for Person Re-identification", Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian, Arxiv 2016.</li>
            <li id="ref33">[33] "Improving Person Re-identification by Attribute and Identity Learning", Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu and Yi Yang, Arxiv 2017.</li>
            <li id="ref34">[34] "Pedestrian Alignment Network for Person Re-identification", Liang Zheng, Zhedong Zheng, Yi Yang, Arxiv 2017.</li>
            <li id="ref35">[35] "Person re-identification by deep joint learning of multi-loss classification", Wei Li, Xiatian Zhu, and Shaogang Gong, IJCAI 2017.</li>
            <li id="ref36">[36] "In Defense of the Triplet Loss for Person Re-Identification", Alexander Hermans, Lucas Beyer and Bastian Leibe, Arxiv 2017.</li>
            <li id="ref37">[37] "Dual Mutual Learning", Ying Zhang, Tao Xiang, Timothy Hospedales, Huchuan Lu, Arxiv 2017.</li>
            <li id="ref38">[38] "Random Erasing Data Augmentation", Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, Arxiv 2017.</li>
            <li id="ref39">[39] "Efficient Online Local Metric Adaptation via Negative Samples for Person Re-Identification", Jiahuan Zhou, Pei Yu, Wei Tang and Ying Wu, ICCV 2017.</li>
            <li id="ref40">[40] "Unsupervised Person Re-identification: Clustering and Fine-tuning", Hehe Fan, Liang Zheng and Yi Yang, Arxiv 2017.</li>
            <li id="ref41">[41] "Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification", Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng, ICCV 2017.</li>
            <li id="ref42">[42] "Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification", Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, Arxiv 2017.</li>
            <li id="ref43">[43] "Constrained Deep Metric Learning for Person Re-identification", Hailin Shi, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Yang Yang, Stan Z. Li, ArXiv 2015.</li>
            <li id="ref44">[44] "An Enhanced Deep Feature Representation for Person Re-identification", Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, Wei-Shi Zheng, WACV 2016.</li>
            <li id="ref45">[45] "Semantics-Aware Deep Correspondence Structure Learning for Robust Person Re-identification", Yaqing Zhang, Xi Li, Liming Zhao, Zhongfei Zhang, IJCAI 2016.</li>
            <li id="ref46">[46] "Human-In-The-Loop Person Re-Identification", Hanxiao Wang, Shaogang Gong, Xiatian Zhu, Tao Xiang, ECCV 2016.</li>
            <li id="ref47">[47] "Covariance descriptor based on bioinspired features for person re-identification and face verification", B. Ma, Y. Su, and F. Jurie, Image and Vision Computing 32 (6), 379-390, 2014 </li>
            <li id="ref48">[48] "Person reidentification using kernel-based metric learning methods", F. Xiong, M. Gou, O. Camps, and M. Sznaier, ECCV 2014.</li>
            <li id="ref49">[49] "Person re-identification by local maximal occurrence representation and metric learning", S. Liao, Y. Hu, X. Zhu, and S. Z. Li, CVPR 2015.</li>
            <li id="ref50">[50] "MARS: A Video Benchmark for Large-Scale Person Re-identification", L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, ECCV 2016.</li>
            <li id="ref51">[51] "Person re-identification in the Wild", L. Zheng, H. Zhang, S. Sun, M. Chandraker, Yi Yang, and Q. Tian, CVPR 2017.</li>
            <li id="ref50">[52] "Query-driven iterated neighborhood graph search for large scale indexing", J. Wang and S. Li, ACM MM 2012.</li>

          </ul>
        <!-- <div id="pr_curve" class="text-center">
            <img src="image/pr_summary.png" class="img-responsive" width="80%" alt="pr_curve image">
          </div>
        <h3>II. Average Precision Rank</h3>
        <table id="AP" class="table table-striped table-bordered" cellspacing="0" width="100%">
        <thead>
            <tr>
                <th style=min-width:90px>AP Rank</th>
                <th style=min-width:160px>Team Name</th>
                <th>Team Member</th>
                <th style=min-width:160px>Average Precision</th>
                <th style=min-width:160px>Institute</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1</td>
                <td>Foo & Bar</td>
                <td>Zheqi He,Yongtao Wang</td>
                <td>0.623447</td>
                <td>Peking University</td>
            </tr>
             <tr>
                <td>2</td>
                <td>NLPR_PAL</td>
                <td>Wenhao He, Fei Yin, Da-Han Wang, Cheng-Lin Liu</td>
                <td>0.560427</td>
                <td>NLPR,CASIA</td>
            </tr>
             <tr>
                <td>3</td>
                <td>gmh</td>
                <td>Minghao Guo</td>
                <td>0.555034</td>
                <td>Tsinghua University</td>
            </tr>
            <tr>
                <td>4</td>
                <td>IVA</td>
                <td>Hao Ye, Yingbin Zheng, Weiyuan Shao, Hong Wang</td>
                <td>0.554721</td>
                <td>Shanghai Advanced Research Institute,CAS</td>
            </tr>
             <tr>
                <td>5</td>
                <td>SCUT_MBCNN</td>
                <td>Jinrong Li, Zijian Zhou, Shuangping Huangs</td>
                <td>0.494374</td>
                <td>South China University of Technology</td>
            </tr>
             <tr>
                <td>6 </td>
                <td>CCFLAB</td>
                <td>Dai Yuchen, Huang Zheng, Gao Yuting</td>
                <td>0.468102</td>
                <td>Shanghai Jiao Tong University</td>
            </tr>
             <tr>
                <td>7</td>
                <td>CAS_HotEye</td>
                <td>Wu Dao, Daipeng Wen</td>
                <td>0.408581</td>
                <td>Instutute of Information Engineering,CAS</td>
            </tr>
             <tr >
                <td >8</td>
                <td >SCUT_DLVC</td>
                <td >Lianwen Jin, Yuliang Liu, Zenghui Sun, Canjie Luo, Zhaohai Li, Lele Xie, Fan Yang</td>
                <td >0.360008</td>
                <td>South China University of Technology</td>
              </tr>
              <tr >
                <td >9</td>
                <td >XMU_SuperLab</td>
                <td >Xiaodong Yang, Li Lin, Yan Zhang, Jinyan Liu, Weiran Li, Bin Jin</td>
                <td >0.351821</td>
                <td >Xiamen University</td>
              </tr>
              <tr >
                <td >10</td>
                <td >Image Research Team</td>
                <td >Long Ma, Lulu Xu, Shenghui Xu</td>
                <td >0.312182</td>
                <td>Sogou Inc.</td>
              </tr>
              <tr style='border-top:4px solid;'>
                <td></td>
                <td >Result of organizing team<a target='_blank' href='https://arxiv.org/abs/1703.06520'>[1]</a></td>
                <td >Minghui Liao</td>
                <td >0.359432</td>
                <td>Mclab</td>
              </tr>
            </tbody>
          </table>



          <h2>Task 2 - End-to-End Recognition Leaderboard</h2>
          <h3>Average Edit-distance Rank</h3>
          <table id="AED" class="table table-striped table-bordered" cellspacing="0" width="100%">
            <thead>
                <tr>
                    <th style=min-width:100px>AED-Rank</th>
                    <th style=min-width:160px>Team Name</th>
                    <th style=min-width:140px>Team Member</th>
                    <th style=min-width:140px>Average Edit Distance</th>
                    <th style=min-width:160px>Institute</th>
                </tr>
            </thead>
            <tbody>
              <tr >
                <td >1</td>
                <td >NLPR_PAL</td>
                <td >Yan-Fei Lv, Wenhao He, Fei Yin, Cheng-Lin Liu</td>
                <td >20.21967368</td>
                <td >NLPR,CASIA</td>
              </tr>
              <tr >
                <td >2</td>
                <td >SCUT_DLVC</td>
                <td >Lianwen Jin, Yuliang Liu, Zenghui Sun, Canjie Luo, Zhaohai Li, Lele Xie, Fan Yang</td>  <td >28.3078742</td>
                <td >South China University of Technology</td>
              </tr>
              <tr >
                <td >3</td>
                <td >CCFLAB</td>
                <td >Dai Yuchen, Huang Zheng, Gao Yuting</td>
                <td >32.129818</td>
                <td >Shanghai Jiao Tong University</td>
              </tr>
              <tr >
                <td >4</td>
                <td >Image Research Team</td>
                <td >Long Ma, Lulu Xu, Shenghui Xu</td>
                <td >35.28943013</td>
                <td >Sogou Inc.</td>
              </tr>
              <tr style='border-top:4px solid;'>
                <td ></td>
                <td >Result of organizing team<a target='_blank' href='https://arxiv.org/abs/1703.06520'>[1]</a><a target='_blank' href='https://arxiv.org/abs/1507.05717'>[2]</a></td>
                <td >Mingkun Yang</td>
                <td >25.62260582</td>
                <td>Mclab</td>
              </tr>

            </tbody>
          </table>

        <h2>References</h2>
        <p>[1] Detecting Oriented Text in Natural Images by Linking Segments. Baoguang Shi, Xiang Bai, Serge Belongie. CVPR 2017. <a t[arget='_blank' href='https://arxiv.org/abs/1703.06520'>pdf</a>]</p>
        <p>[2] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. Baoguang Shi, Xiang Bai, Cong Yao. PAMI 2017.<a t[arget='_blank' href='https://arxiv.org/abs/1507.05717'>pdf</a>] </p> -->
    	<div id="footer"> </div>
    </div>

  </body>
</html>
